{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/30 15:12:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/12/30 15:12:47 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "curr_path = str(Path(os.getcwd()).parent)\n",
    "sys.path.append(curr_path)\n",
    "from scripts.sa2_age_allocation import *\n",
    "from scripts.constants import *\n",
    "from scripts.load import *\n",
    "from scripts.transform import *\n",
    "from scripts.read import *\n",
    "from scripts.misc_changes import *\n",
    "from scripts.join import *\n",
    "from scripts.plotting import *\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.column import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, LogisticRegression\n",
    "from pyspark.ml.regression import DecisionTreeRegressor, LinearRegression, RandomForestRegressor, LinearRegressionModel\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import geopandas as gpd\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "spark = create_spark()\n",
    "PREFIX = \".\"\n",
    "# from scripts.model_linreg import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "consumer_external = spark.read.parquet(\"../data/curated/consumer_external_join_age_allocated.parquet\")\n",
    "tbl_merchants = read_curated_tbl_merchant(spark, PREFIX)\n",
    "merchant_fraud = spark.read.csv(\"../data/curated/merchant_fraud_probability.csv/\", header=True)\n",
    "transactions = spark.read.parquet(\"../data/raw/raw_transactions/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transactions_drop_columns(df):\n",
    "    # Drop Male and Female Age coluns\n",
    "    gender_age_cols = [col for col in df.columns if \"male\" in col]\n",
    "    gender_cols = [col for col in df.columns if f\"{GENDER}_\" in col]\n",
    "    \n",
    "    revenue_level_cols = [col for col in df.columns if REVENUE_LEVEL in col]\n",
    "    revenue_level_cols = sorted(revenue_level_cols, key=lambda x:len(x))[1:]\n",
    "\n",
    "    # Order cols\n",
    "    order_year_cols = [col for col in df.columns if ORDER_YEAR in col]\n",
    "    order_year_cols = sorted(order_year_cols, key=lambda x:len(x))[1:]\n",
    "\n",
    "    order_year_cols = [col for col in df.columns if ORDER_YEAR in col]\n",
    "    order_year_cols = sorted(order_year_cols, key=lambda x:len(x))[1:]\n",
    "\n",
    "    order_year_cols = [col for col in df.columns if ORDER_YEAR in col]\n",
    "    order_year_cols = sorted(order_year_cols, key=lambda x:len(x))[1:]\n",
    "\n",
    "    order_year_cols = [col for col in df.columns if ORDER_YEAR in col]\n",
    "    order_year_cols = sorted(order_year_cols, key=lambda x:len(x))[1:]\n",
    "\n",
    "    order_month_cols = [col for col in df.columns if ORDER_MONTH in col]\n",
    "    order_month_cols = sorted(order_month_cols, key=lambda x:len(x))[1:]\n",
    "\n",
    "    order_dom_cols = [col for col in df.columns if ORDER_DAY_OF_MONTH in col]\n",
    "    order_dom_cols = sorted(order_dom_cols, key=lambda x:len(x))[1:]\n",
    "\n",
    "    order_dow_cols = [col for col in df.columns if ORDER_DAY_OF_WEEK in col]\n",
    "    order_dow_cols = sorted(order_dow_cols, key=lambda x:len(x))[1:]\n",
    "\n",
    "    state_cols = [col for col in df.columns if f\"{STATE_NAME}_\" in col]\n",
    "    sa2_cols = [col for col in df.columns if SA2_NAME in col]\n",
    "\n",
    "    earningsum_cols = sorted([col for col in df.columns if \"earningsum\" in col])[:-1]\n",
    "    medianage_cols = sorted([col for col in df.columns if \"median_age\" in col])[:-1]\n",
    "    earningmedian_cols = sorted([col for col in df.columns if \"earningmedian\" in col])[:-1]\n",
    "    earningmean_cols = sorted([col for col in df.columns if \"eariningmean\" in col])[:-1]\n",
    "    earners_cols = sorted([col for col in df.columns if \"earners\" in col])[:-1]\n",
    "    industry_tag_cols = sorted([col for col in df.columns if \"industry_tag\" in col])[1:]\n",
    "    additional_drop = gender_age_cols + earningmedian_cols + medianage_cols + earningsum_cols +\\\n",
    "        earningmean_cols + earners_cols + [RATIO, SA2_CODE_2016, NAME] + industry_tag_cols +\\\n",
    "            order_dom_cols + order_dow_cols + order_month_cols + order_year_cols + \\\n",
    "                gender_age_cols + revenue_level_cols + gender_cols + state_cols + sa2_cols\\\n",
    "    \n",
    "    return tuple(additional_drop)\n",
    "\n",
    "def consumer_external_drop_columns(df):\n",
    "    # Drop Male and Female Age coluns\n",
    "    gender_age_cols = [col for col in df.columns if \"male\" in col]\n",
    "    gender_cols = [col for col in df.columns if f\"{GENDER}_\" in col]\n",
    "    \n",
    "    revenue_level_cols = [col for col in df.columns if REVENUE_LEVEL in col]\n",
    "    revenue_level_cols = sorted(revenue_level_cols, key=lambda x:len(x))[1:]\n",
    "\n",
    "    # Order cols\n",
    "    order_year_cols = [col for col in df.columns if ORDER_YEAR in col]\n",
    "    order_year_cols = sorted(order_year_cols, key=lambda x:len(x))[1:]\n",
    "\n",
    "    order_year_cols = [col for col in df.columns if ORDER_YEAR in col]\n",
    "    order_year_cols = sorted(order_year_cols, key=lambda x:len(x))[1:]\n",
    "\n",
    "    order_year_cols = [col for col in df.columns if ORDER_YEAR in col]\n",
    "    order_year_cols = sorted(order_year_cols, key=lambda x:len(x))[1:]\n",
    "\n",
    "    order_year_cols = [col for col in df.columns if ORDER_YEAR in col]\n",
    "    order_year_cols = sorted(order_year_cols, key=lambda x:len(x))[1:]\n",
    "\n",
    "    order_month_cols = [col for col in df.columns if ORDER_MONTH in col]\n",
    "    order_month_cols = sorted(order_month_cols, key=lambda x:len(x))[1:]\n",
    "\n",
    "    order_dom_cols = [col for col in df.columns if ORDER_DAY_OF_MONTH in col]\n",
    "    order_dom_cols = sorted(order_dom_cols, key=lambda x:len(x))[1:]\n",
    "\n",
    "    order_dow_cols = [col for col in df.columns if ORDER_DAY_OF_WEEK in col]\n",
    "    order_dow_cols = sorted(order_dow_cols, key=lambda x:len(x))[1:]\n",
    "\n",
    "    state_cols = [col for col in df.columns if f\"{STATE_NAME}_\" in col]\n",
    "    sa2_cols = [col for col in df.columns if SA2_NAME in col]\n",
    "\n",
    "    earningsum_cols = sorted([col for col in df.columns if \"earningsum\" in col])\n",
    "    medianage_cols = sorted([col for col in df.columns if \"median_age\" in col])\n",
    "    earningmedian_cols = sorted([col for col in df.columns if \"earningmedian\" in col])\n",
    "    earningmean_cols = sorted([col for col in df.columns if \"eariningmean\" in col])\n",
    "    earners_cols = sorted([col for col in df.columns if \"earners\" in col])\n",
    "    industry_tag_cols = sorted([col for col in df.columns if \"industry_tag\" in col])\n",
    "    additional_drop = gender_age_cols + earningmedian_cols + medianage_cols + earningsum_cols +\\\n",
    "        earningmean_cols + earners_cols + [RATIO, SA2_CODE_2016, NAME] + industry_tag_cols +\\\n",
    "            order_dom_cols + order_dow_cols + order_month_cols + order_year_cols + \\\n",
    "                gender_age_cols + revenue_level_cols + gender_cols + state_cols + sa2_cols\\\n",
    "    \n",
    "    return tuple(additional_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merchants = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tbl_merchants = spark.read.parquet(\"../data/curated/tbl_merchants.parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # file imports\n",
    "# transactions = spark.read.parquet(PREFIX+RAW_TRANSACTIONS_PATH)\n",
    "# transactions = round_dollar_values(transactions)\n",
    "# transactions = transactions.join(consumer_external, on=USER_ID, how=INNER_JOIN)\n",
    "# transactions = transactions.join(tbl_merchants, on=MERCHANT_ABN, how=INNER_JOIN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(transactions_1.count())\n",
    "# print(transactions_2.count())\n",
    "# print(transactions_3.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_transactions = read_curated_transactions_all(spark, PREFIX)\n",
    "# all_transactions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CURATED_TRANSACTIONS_ALL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_test_files(consumer_external: DataFrame):\n",
    "    JOIN_COLUMNS = [USER_ID, SA2_CODE, STATE, POSTCODE, GENDER]\n",
    "    INDUSTRY_1 = \"tent awning\"\n",
    "    INDUSTRY_2 = \"gift card novelty souvenir\"\n",
    "    INDUSTRY_3 = \"digital goods books movies music\"\n",
    "    industries = [INDUSTRY_1, INDUSTRY_2, INDUSTRY_3]\n",
    "    order_cols = (col(ORDER_YEAR), col(ORDER_MONTH), col(ORDER_DAY_OF_MONTH))\n",
    "    INDEXED_COL = \"_indexed\"\n",
    "    cat_cols = [MERCHANT_ABN, SA2_CODE, STATE, USER_ID, GENDER, ALLOCATED_AGES, INDUSTRY_TAGS]\n",
    "\n",
    "    print(\"IMPORT ALL TRANSACTION FILES\")\n",
    "    transactions_all_files = [f\"{PREFIX}{CURATED_TRANSACTIONS_ALL_PATH}{fname}\" for fname in os.listdir(PREFIX+CURATED_TRANSACTIONS_ALL_PATH)]\n",
    "\n",
    "    # First 6 files are train, next 3 are test\n",
    "    print(\"DEFINE TRAIN AND TEST FILES\")\n",
    "    train_data_files = transactions_all_files[0:9]\n",
    "    # train_data_files = transactions_all_files[0:6]\n",
    "    # validate_data_files = transactions_all_files[6:9]\n",
    "    test_data_files = transactions_all_files[9:12]\n",
    "    \n",
    "    print(\"DROP IRRELEVANT COLUMNS FOR CONSUMER EXTERNAL\")\n",
    "    drop_cols = consumer_external_drop_columns(consumer_external)\n",
    "    consumer_external = consumer_external.drop(*(drop_cols))\n",
    "\n",
    "    # train_data_files\n",
    "    print(\"READ TRAIN/TEST FILES\")\n",
    "    train_df = spark.read.parquet(*train_data_files)\n",
    "    # validate_df = spark.read.parquet(*validate_data_files)\n",
    "    test_df = spark.read.parquet(*test_data_files)\n",
    "\n",
    "    # Now save the model\n",
    "    print(\"PERFORM CLEANING/TRANSFORMATIONS FOR TRAIN\")\n",
    "    drop_cols = transactions_drop_columns(train_df)\n",
    "    test_path = \"./test_files/model_save_test/\"\n",
    "    train_df = train_df.drop(*drop_cols)\n",
    "    train_df = cast_data_type(train_df)\n",
    "    train_df = encode_revenue_level(train_df)\n",
    "    train_df = train_df.withColumns({\n",
    "        USER_ID: col(USER_ID).cast(StringType()),\n",
    "        MERCHANT_ABN: col(MERCHANT_ABN).cast(StringType())\n",
    "    })\n",
    "    \n",
    "    # print(\"PERFORM CLEANING/TRANSFORMATIONS FOR VALIDATE\")\n",
    "    # validate_df = validate_df.drop(*drop_cols)\n",
    "    # validate_df = cast_data_type(validate_df)\n",
    "    # validate_df = encode_revenue_level(validate_df)\n",
    "    # validate_df = validate_df.withColumns({\n",
    "    #     USER_ID: col(USER_ID).cast(StringType()),\n",
    "    #     MERCHANT_ABN: col(MERCHANT_ABN).cast(StringType())\n",
    "    # })\n",
    "    \n",
    "    print(\"PERFORM CLEANING/TRANSFORMATIONS FOR TEST\")\n",
    "    test_df = test_df.drop(*drop_cols)\n",
    "    test_df = cast_data_type(test_df)\n",
    "    test_df = encode_revenue_level(test_df)\n",
    "    test_df = test_df.withColumns({\n",
    "        USER_ID: col(USER_ID).cast(StringType()),\n",
    "        MERCHANT_ABN: col(MERCHANT_ABN).cast(StringType())\n",
    "    })\n",
    "    \n",
    "    print(\"PERFORM JOINING TO CONSUMER ROWS\")\n",
    "    train_transactions = train_df.join(consumer_external, on=JOIN_COLUMNS, how=INNER_JOIN)\n",
    "    # validate_transactions = validate_df.join(consumer_external, on=JOIN_COLUMNS, how=INNER_JOIN)\n",
    "    test_transactions = test_df.join(consumer_external, on=JOIN_COLUMNS, how=INNER_JOIN)\n",
    "    \n",
    "    print(\"PERFORM DELETION OF SPECIFIC COLUMNS\")\n",
    "    train_transactions = train_transactions.drop(ORDER_ID, POSTCODE, ORDER_DAY_OF_WEEK)\n",
    "    # validate_transactions = validate_transactions.drop(ORDER_ID, POSTCODE, ORDER_DAY_OF_WEEK)\n",
    "    test_transactions = test_transactions.drop(ORDER_ID, POSTCODE, ORDER_DAY_OF_WEEK)\n",
    "    \n",
    "    print(\"PERFORM FILTERING OF TOP 3 INDUSTRIES\")\n",
    "    train_transactions = train_transactions.where(col(INDUSTRY_TAGS).isin(industries)).orderBy(*order_cols)\n",
    "    # validate_transactions = validate_transactions.where(col(INDUSTRY_TAGS).isin(industries)).orderBy(*order_cols)\n",
    "    test_transactions = test_transactions.where(col(INDUSTRY_TAGS).isin(industries)).orderBy(*order_cols)\n",
    "    \n",
    "    df_columns = train_transactions.columns\n",
    "    \n",
    "    print(\"PERFORM STRING INDEXING\")\n",
    "    for column in cat_cols:\n",
    "        col_indexer = StringIndexer(inputCol=column, outputCol=column+INDEXED_COL, handleInvalid=\"keep\")\n",
    "        df_columns.append(column+INDEXED_COL)\n",
    "        fitted_indexer = col_indexer.fit(train_transactions)\n",
    "        train_transactions = fitted_indexer.transform(train_transactions)\n",
    "        # validate_transactions = fitted_indexer.transform(validate_transactions)\n",
    "        test_transactions = fitted_indexer.transform(test_transactions)\n",
    "\n",
    "    df_columns = [col for col in df_columns if col not in cat_cols]\n",
    "    df_columns = [col for col in df_columns if col != DOLLAR_VALUE]\n",
    "    print(df_columns)\n",
    "    \n",
    "    # print(\"PERFORM VECTOR ASSEMBLING\")\n",
    "    # assembler = VectorAssembler(inputCols=df_columns, outputCol=\"features\")\n",
    "    # train_vec_transactions = assembler.transform(train_transactions)\n",
    "    # # validate_vec_transactions = assembler.transform(validate_transactions)\n",
    "    # test_vec_transactions = assembler.transform(test_transactions)\n",
    "\n",
    "    # return train_vec_transactions, test_vec_transactions\n",
    "    return\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_hyperparams_tuning_lr(train, test):\n",
    "    LEARNING_RATE = 0.01\n",
    "    L1_PENALTY = 0.1\n",
    "    NUM_ITERS = 100\n",
    "    EPSILON = 0.000001\n",
    "    mae_scores = []\n",
    "\n",
    "    # Hyperparamters to tune for\n",
    "    REG_PARAMS = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "    ELASTIC_NET_PARAMS = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "    \n",
    "    for reg in REG_PARAMS:\n",
    "        for net in ELASTIC_NET_PARAMS:\n",
    "            print(f\"Reg: {reg}, Net: {net}\")\n",
    "            lr = LinearRegression(featuresCol=\"features\", labelCol=DOLLAR_VALUE, \n",
    "                            predictionCol=f\"predicted_{DOLLAR_VALUE}\",\n",
    "                            maxIter=100, regParam=reg, elasticNetParam=net)\n",
    "            lr_model = lr.fit(train)\n",
    "            predictions = lr_model.transform(test)\n",
    "            \n",
    "            evaluator = RegressionEvaluator(labelCol=DOLLAR_VALUE, predictionCol=f\"predicted_{DOLLAR_VALUE}\", metricName=\"mae\")\n",
    "            score = evaluator.evaluate(predictions)\n",
    "            mae_scores.append(score)\n",
    "            print(f\"Score: {score}\")\n",
    "    \n",
    "    return mae_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_linear_regression(train, test, params: dict()):\n",
    "\n",
    "    print(\"BEGIN TRAINING MODEL\")\n",
    "    lr = LinearRegression(featuresCol=\"features\", labelCol=DOLLAR_VALUE, \n",
    "                            predictionCol=f\"predicted_{DOLLAR_VALUE}\",\n",
    "                            regParam=params[\"reg\"], elasticNetParam=params[\"net\"])\n",
    "    lr_model = lr.fit(train)\n",
    "    print(\"PRODUCE PREDICTIONS\")\n",
    "    predictions = lr_model.transform(test)\n",
    "\n",
    "    return lr_model, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec, test_vec = prepare_train_test_files(consumer_external)\n",
    "# prepare_train_test_files(consumer_external)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_scores = perform_hyperparams_tuning_lr(train_vec, test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"reg\": 100, \"net\": 0.25}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model, predictions = run_linear_regression(train_vec, test_vec, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(labelCol=DOLLAR_VALUE, predictionCol=f\"predicted_{DOLLAR_VALUE}\", metricName=\"r2\")\n",
    "score = evaluator.evaluate(predictions)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTED_DOLLARS = \"predicted_dollar_value\"\n",
    "RESIDUALS = \"residuals\"\n",
    "pred_values = predictions.select(DOLLAR_VALUE, f\"predicted_{DOLLAR_VALUE}\")\n",
    "pred_values = pred_values.withColumn(PREDICTED_DOLLARS, round(PREDICTED_DOLLARS, 2))\n",
    "pred_values = pred_values.withColumn(RESIDUALS, col(DOLLAR_VALUE) - col(PREDICTED_DOLLARS))\n",
    "pred_values.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_values, sample_values = pred_values.randomSplit([0.995, 0.005])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_values.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = list(sample_values.select(RESIDUALS).rdd.flatMap(lambda x: x).collect())\n",
    "predicted_dollar = list(sample_values.select(PREDICTED_DOLLARS).rdd.flatMap(lambda x: x).collect())\n",
    "actual_dollar = list(sample_values.select(DOLLAR_VALUE).rdd.flatMap(lambda x: x).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_values.select(PREDICTED_DOLLARS).count()\n",
    "predicted_dollar = list(sample_values.select(PREDICTED_DOLLARS).rdd.flatMap(lambda x: x).collect())\n",
    "len(predicted_dollar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(actual_dollar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the standard deviation of residuals\n",
    "residual_std = np.std(residuals)\n",
    "\n",
    "# Calculate standardized residuals\n",
    "standardized_residuals = residuals / residual_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot of residuals against predicted values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(predicted_dollar, actual_dollar, alpha=0.5, c=\"#C76528\")\n",
    "plt.title(\"Predicted vs Actual Dollar Values\")\n",
    "plt.xlabel(\"Predicted values ($)\")\n",
    "plt.ylabel(\"Actual values ($)\")\n",
    "plt.savefig(\"../plots/predicted_vs_actual_dollars.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot of residuals against predicted values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(predicted_dollar, standardized_residuals, alpha=0.5, c=\"#C76528\")\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.title(\"Predicted Dollar Values vs. Residuals\")\n",
    "plt.xlabel(\"Predicted values ($)\")\n",
    "plt.ylabel(\"Standardised Residuals\")\n",
    "plt.savefig(\"../plots/predicted_vs_residuals.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_transactions = joined_transactions.drop(ORDER_ID, POSTCODE, ORDER_DAY_OF_WEEK)\n",
    "INDUSTRY_1 = \"tent awning\"\n",
    "INDUSTRY_2 = \"gift card novelty souvenir\"\n",
    "INDUSTRY_3 = \"digital goods books movies music\"\n",
    "industries = [INDUSTRY_1, INDUSTRY_2, INDUSTRY_3]\n",
    "INDEXED_COL = \"_indexed\"\n",
    "cat_cols = [MERCHANT_ABN, SA2_CODE, STATE, USER_ID, GENDER, ALLOCATED_AGES, INDUSTRY_TAGS]\n",
    "\n",
    "joined_transactions = joined_transactions.where(col(INDUSTRY_TAGS).isin(industries)).orderBy(col(ORDER_YEAR), col(ORDER_MONTH), col(ORDER_DAY_OF_MONTH))\n",
    "# transactions_1 = joined_transactions.where(col(INDUSTRY_TAGS) == INDUSTRY_1).orderBy(col(ORDER_YEAR), col(ORDER_MONTH), col(ORDER_DAY_OF_MONTH))\n",
    "# transactions_2 = joined_transactions.where(col(INDUSTRY_TAGS) == INDUSTRY_2).orderBy(col(ORDER_YEAR), col(ORDER_MONTH), col(ORDER_DAY_OF_MONTH))\n",
    "# transactions_3 = joined_transactions.where(col(INDUSTRY_TAGS) == INDUSTRY_3).orderBy(col(ORDER_YEAR), col(ORDER_MONTH), col(ORDER_DAY_OF_MONTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_transactions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_cols = (col(ORDER_YEAR), col(ORDER_MONTH), col(ORDER_DAY_OF_MONTH))\n",
    "joined_transactions.orderBy(*order_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns = joined_transactions.columns\n",
    "for column in cat_cols:\n",
    "    col_indexer = StringIndexer(inputCol=column, outputCol=column+INDEXED_COL)\n",
    "    df_columns.append(column+INDEXED_COL)\n",
    "    joined_transactions = col_indexer.fit(joined_transactions).transform(joined_transactions)\n",
    "\n",
    "df_columns = [col for col in df_columns if col not in cat_cols]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=df_columns, outputCol=\"features\")\n",
    "vector_transactions = assembler.transform(joined_transactions)\n",
    "\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=DOLLAR_VALUE, \n",
    "                          predictionCol=f\"predicted_{DOLLAR_VALUE}\")\n",
    "lr_model = lr.fit(vector_transactions)\n",
    "\n",
    "mae_evaluator = RegressionEvaluator(labelCol=DOLLAR_VALUE, predictionCol=f\"predicted_{DOLLAR_VALUE}\", metricName=\"mae\")\n",
    "r2_evaluator = RegressionEvaluator(labelCol=DOLLAR_VALUE, predictionCol=f\"predicted_{DOLLAR_VALUE}\", metricName=\"r2\")\n",
    "mae_score = mae_evaluator.evaluate(predictions)\n",
    "r2_score = r2_evaluator.evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol=\"features\", labelCol=DOLLAR_VALUE, \n",
    "                          predictionCol=f\"predicted_{DOLLAR_VALUE}\")\n",
    "lr_model = lr.fit(vector_transactions)\n",
    "\n",
    "mae_evaluator = RegressionEvaluator(labelCol=DOLLAR_VALUE, predictionCol=f\"predicted_{DOLLAR_VALUE}\", metricName=\"mae\")\n",
    "r2_evaluator = RegressionEvaluator(labelCol=DOLLAR_VALUE, predictionCol=f\"predicted_{DOLLAR_VALUE}\", metricName=\"r2\")\n",
    "mae_score = mae_evaluator.evaluate(predictions)\n",
    "r2_score = r2_evaluator.evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit_linreg(train_df=all_transactions, save_path=SAVE_PATH, target_col=DOLLAR_VALUE, drop_cols=drop_cols, prefix=PREFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_transactions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_model_linreg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_allocation = spark.read.parquet(\"../data/curated/consumer_external_join_age_allocated.parquet\")\n",
    "male_consumers = age_allocation.where(col(GENDER) == MALE) \n",
    "female_consumers = age_allocation.where(col(GENDER) == FEMALE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_count = male_consumers.groupBy(SA2_CODE, ALLOCATED_AGES).count()\n",
    "female_count = female_consumers.groupBy(SA2_CODE, ALLOCATED_AGES).count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mast30034",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
